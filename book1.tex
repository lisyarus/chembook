\documentclass{article}

% for dots in table of contents
\usepackage{tocloft}
\renewcommand{\cftsecdotsep}{\cftdotsep}

% for clickable table of contents
\usepackage{hyperref}
\hypersetup{
	colorlinks = true,
	linkcolor = blue,
	citecolor = magenta,
}

% for appendices in table of contents
\usepackage[titletoc]{appendix}

% for references section in table of contents
\usepackage[nottoc]{tocbibind}

\usepackage[a4paper,margin=1in]{geometry}

\newcommand{\molecule}[1]{\textrm{#1}}

% for nice monospaced font using \texttt
\usepackage{courier}

\usepackage{enumitem}

\usepackage{titlesec}

\usepackage{amsmath}

\setlength{\parskip}{0.4em}

\title{Quantum Chemistry In One Weekend}
\author{Nikita Lisitsa \\ \texttt{lisyarus@gmail.com}}
\date{circa February 2020}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{What it is all about}

It was already a month of me trying to come up with a perfect introduction for this book when I decided I better do something more practical. Let's get straight to the point: what is this book about? Or, rather, what is it \textit{not} about? This is \textit{not} a

\begin{itemize}
\item Chemistry textbook
\item Quantum mechanics textbook
\item Linear algebra textbook
\item Python tutorial
\end{itemize}

But what it \textit{is}, then? Well, a sturdy mix of all the above, aimed at getting physically significant results as soon as possible, yet without sacrificing understanding. We are going to write some Python code using \href{https://numpy.org}{\texttt{numpy}} and \href{https://www.scipy.org}{\texttt{scipy}} libraries (and ocasionally \href{https://matplotlib.org}{\texttt{matplotlib}}, if you want some fancy plots \& images) and obtain energies and orbital densities for some pretty tiny chemical systems (that is, systems composed of nuclei \& electrons). You will also most probably need a file called \texttt{gaussian.py} which you can download directly from \href{https://raw.githubusercontent.com/lisyarus/qchem/master/gaussian.py}{my github}.

The reader is expected to have \textit{at least} a bit of knowledge on physics/chemistry, basic mathematics and Python programming language. For getting into the physics and maths involved, I recommend either reading the first few chapters of \cite{ref:atkins} or at least not skipping the \nameref{sec:var} section. For learning Python, I recommend googling some tutorials/courses/etc, for there are way too many of them to recommend anything in particular.

The following few sections briefly discuss some essential theoretical background, starting with basic mechanics. I'd especially recommend reading the \nameref{sec:var} section so that you understand what all those matrices even mean and what those solvers are actually solving. The actual coding starts at the \nameref{sec:harmonic} section.

Beware that I am by no means a scientist, a quantum chemist or even a physicist. I am barely an amateur possessing some unconventional hobbies. The methods that we are going to make use of are \textbf{not} the methods used by real quantum chemical software and, as far as I understand, are only applicable to very tiny systems (with, say, 4 or 5 electrons, while systems of practical interest can contain \textit{thousands} of them). This book only covers one-electron systems.

Finally, feel free to \href{mailto:lisyarus@gmail.com}{email me} if you have some questions, I would love to be of help. Oh, and \href{https://twitter.com/lisyarus}{follow me} on twitter, I post weird simulations/computations/etc sometimes.

\bigbreak
\textit{The name of this book was inspired by Peter Shirley's \href{https://raytracing.github.io}{Ray Tracing in One Weekend} series, which is an amazing source for learning about modern photorealistic rendering.}

\newpage

\section{A tale of energies}

So, what we are going to do is compute the lowest-energy states of atoms \& ions. Why is energy so important?

In any decent physical framework (e.g. classical or quantum mechanics) total energy of a closed system is conserved. It consists of several parts (e.g. kinetic + potential) which don't necessarily remain constant, but conservation of total energy implies that decrease in one part leads to increase in another. Consider a perfectly reasonable situation of a rock falling from sky: the higher it is, the higher is the potential energy; as the rock falls down, the potential energy decreases, leading to increase in kinetic energy, i.e. acceleration (some people call this \textit{gravity}).

We know that even if the rock was thrown in up the air, it will eventually fall down anyway. This is how all mechanical systems behave: they accelerate towards areas of less potential energy (this is Newton's second law). This doesn't generally mean that the system eventually falls into a local minimum of potential energy. Remember that as potential energy decreases, the kinetic energy increases, so upon reaching a local minimum the system would run with insane speeds and miss the minimum completely. This doesn't happen to rocks, though, since energy is converted into stresses in the ground when rock finally reachs it, but this does happen to pendulums: the potential energy minimum is when the pendulum is at its lowest point, yet the pendulum flies through it at high speed.

A system can, however, reach a local minimum of potential energy if it has some energy sink that can absorb excess kinetic energy. Typically, this sink is just the environment: a particle constantly hitting another particles quickly loses excess kinetic energy and reachs so-called thermal equilibrium. Recall that temperature is simply average kinetic energy; thus, the lower the surrounding temperature, the higher are the chances the the system will be found in a local minimum of potential energy. And \textit{this} effect is the major reason why we seek for lowest-energy states: most molecules in real life are in those states supplemented with small kinetic fluctuations (real molecules aren't static, they wiggle and bend randomly).

Interactions can likewise be described in terms of energy: attractive force between particles means that the closer they are, the lower the energy, and repulsive force means the opposite. A \textit{bond} happens when the interaction between two systems (say, particles) has a local minimum of potential energy at some specific distance between the systems. The energy of the bond is the difference between the energy of two systems combined when the bonding takes place and the energy when they are so far that no interaction is present at all. Thus, creating a bond releases energy, while breaking it requires energy. This, coupled with some termodynamics, leads to equations that allow us to predict temperature changes in chemical reactions, provided we know the energies of the bonds. By the way, a chemical reaction is a process of forming and breaking chemical bonds. Bonds are called chemical is they happen between atomic nuclei and electrons, and usually involve at least three participants (two nuclei and an electron holding those together). As an extreme example, a piece of metal can be considered to be a single bond between all particles involved (this is so-called \textit{metallic bonding}).

I think I have convinced the reader that energies are of great utility and importance. They allow us to predict which reactions can happen and which cannot, and at what temperature. They can let us visualize how molecules look like, i.e. what is the geometry of the molecules. What is especially nice is that chemists can actually observe the energies directly, --- or, rather, the differences between them, --- simply by looking at the molecules at some special conditions: this is called \textit{spectroscopy}. Well, okay, the word ``simply'' was slightly unfair, but the point is that there are methods that can verify energy calculations with great precision.

\newpage

\section{A classical failure}

One of the first things I tried to implement when I felt sufficiently self-confident about my skills in 3D graphics and simulations was an \(\molecule{H}_2\) molecule. It failed miserably: most configurations were disastrous, with a few exceptions at unstable equilibria. There was no bonding, no stability, no \textit{molecule} being formed.

It turns out I've rediscovered one of the many problems of XIX-century physics: it fails to explain how atoms \& molecules work. There are many reasons for that, the most well-known are

\begin{itemize}
\item \href{https://en.wikipedia.org/wiki/Earnshaw's_theorem}{Earnshaw's theorem}: a system of charged particles cannot reach stable equilibrium by means of electrostatic forces alone, --- this is exactly what I've faced when trying to simulate those
\item Electromagnetic radiation: an electron orbiting a nucleus would lose energy by \href{https://en.wikipedia.org/wiki/Larmor_formula}{emitting electromagnetic waves}, thus eventually falling on the nucleus
\item Discrete emission spectrum: it was observed via spectrometry that the possible energy levels of hydrogen are \href{https://en.wikipedia.org/wiki/Hydrogen_spectral_series}{discrete}, although classical mechanics always predicts a continuum of possible energy levels
\end{itemize}

Quantum mechanics came to the rescue. The advantage of the new theory is that is correctly describes \textit{a ridiculous number of previously unexplained phenomena}, the disadvantage being that the mathematics involed is rather hard. In classical mechanics, to predict the motion of a particle, you would solve an \textit{ordinary differential equation} --- pick up your favourite method (4th-order Runge-Kutta seemingly being the most popular one) and you are done. In quantum mechanics, you have to solve a \textit{hyperbolic partial differential equation}, --- which are notoriously hard to solve numerically, --- not to mention that we don't have the notions of exact particle's posisition or momentum anymore.

Gladly, being difficult is very far from being impossible, and we are going to employ this nice observation.

\textit{N.B. The theory of relativity plays an important role as well, though in chemistry it is mostly responsible for much finer effects. It also limits the applicability of non-relativistic quantum mechanics to atoms with atomic number being 137 and higher (as of today, only 118 have been identified as existing), and I've written a \href{https://twitter.com/lisyarus/status/1189227697605660673?s=20}{twitter thread} on the details of this.}

\newpage

\section{Quantum mechanics: quick and dirty} \label{sec:qm}

\textit{From now on, we'll start being more technical.}

The reader will learn from any inroductory material on quantum mechanics (e.g. \cite{ref:atkins}) that in Schrödinger representation everything boils down to the \textit{time-independent Schrödinger equation}:

\begin{equation} \label{eq:shrod} \hat H \psi = E \psi \end{equation}

Physicists and chemists use the \(\widehat{hat}\) to distinguish operators \(\hat H\) from numbers \(E\). This number \(E\) is the energy of the system. It isn't always well-defined (there may be some probabilities involved), but we are interested in cases when it is. \(\psi\) is the \textit{wavefunction}, which encodes everything about the state of our physical system. Technically, it is a vector from some Hilbert space, but in many real situations this Hilbert space consists of functions, so \(\psi\) is just a function, like \(\psi(x) = \sin(3x)\). The unknowns of this equation are usually both \(\psi\) and \(E\).

In quantum mechanics, every measurable physical property is encoded as a linear operator. Energy is no exception: \(\hat H\) is the energy operator, also called \textit{the Hamiltonian} (in honour of William Rowan Hamilton, who has nothing to do with quantum mechanics\footnote{He did however make significant contribution to \textit{classical} mechanics, presenting the so-called \textit{Hamiltonian formalism}, where the Hamiltonian is the energy function that corresponds directly to operator \(\hat H\).}, in accordance with the Arnold Principle). The energy operator is arguably the most important one: after all, it governs the evolution of the system. All we need to know to start reasoning about a system is the energy operator (and the Hilbert space it operates on, though purists claim this piece of data is built into the operator itself, and I tend to agree). In our cases, this operator applies to functions, usually by multiplying them by other functions, or differentiating them, or both, e.g. \(\hat H \psi = -\frac{1}{2} \frac{d^2}{dx^2}\psi\). Out or pure curiosity, let us apply this operator to the \(\psi\) from previous paragraph:

\begin{equation} \hat H \psi = -\frac{1}{2} \frac{d^2}{dx^2} \sin(3x) = \frac{9}{2} \sin(3x) \end{equation}

because \(\frac{d^2}{dx^2} \sin(kx) = -k^2 \sin(kx)\). We can see that for such \(\hat H\), \(\psi\) is a solution of \eqref{eq:shrod}. Such sine-like wavefunctions are called \textit{plane waves}, while the physical system described by the operator \(\hat H\) is called a \textit{free one-dimensional particle}. In this cases, we call \(\psi\) the \textit{eigenfunction} or \textit{eigenvector} of the operator \(\hat H\), and \(E\) is the corresponding \textit{eigenvalue}.

Let us repeat:
\begin{itemize}
\item A system (say, a water molecule) is described by an operator \(\hat H\)
\item A specific state of a system (a water molecule at that point, flying in that direction with that speed) is described by a wavefunction \(\psi\) (we'll ocasionally use other letters for it, but always greek ones)
\item We seek for solutions of the equation \(\hat H \psi = E \psi\), where \(\hat H\) is known, while both \(E\) and \(\psi\) are unknowns. 
\end{itemize}

It so happens in physical systems that the possible values of \(E\) are bounded below (see \href{https://www.themathcitadel.com/energy-levels-of-molecules-are-bounded-below/}{here} for a proof in case of chemical systems). If the minimal possible value of \(E\) is attained by some wavefunction \(\psi\), we call it the \textit{ground state}. It is the state of most importance, since in general it is the most likely state a random system will be found in (the lower the temperature, the more the probability of ground state, see \href{https://en.wikipedia.org/wiki/Boltzmann_distribution}{Gibbs distribution}).

\textit{N.B. There are severe technical difficulties in actual mathematical formalism of quantum mechanics. They can be overcome, but the mathematics involved is far beyond the scope of this book. You can find a reasonably graceful introduction to the matter in \cite{ref:hall}.}

\newpage

\section{Variational method} \label{sec:var}
\section{Practical considerations}
\section{Harmonic oscillator} \label{sec:harmonic}
\section{Hydrogen atom \(\molecule{H}\)}
\section{Helium cation \(\molecule{He}^+\)} 
\section{Lithium 2+ cation \(\molecule{Li}^{2+}\)}
\section{Dihydrogen cation \(\molecule{H}_2^+\)}
\section{Trihydrogen 2+ cation \(\molecule{H}_3^{2+}\)}

\begin{thebibliography}{9}
\bibitem{ref:atkins}
	Peter Atkins, Ronald Friedman,
	\textit{Molecular Quantum Mechanics}
\bibitem{ref:lieb}
	Elliot Lieb,
	\textit{The Stability of Matter: From Atoms to Stars}
\bibitem{ref:hall}
	Brian Hall,
	\textit{Quantum Theory for Mathematicians}
\bibitem{ref:kato-th}
	Tosio Kato,
	\textit{On the eigenfunctions of many-particle systems in quantum mechanics}
\bibitem{ref:kato-ham}
	Tosio Kato,
	\textit{Fundamental properties of Hamiltonian operators of Schrödinger type}
\bibitem{ref:fulton-harris}
	William Fulton, Joe Harris,
	\textit{Representation Theory: A First Course}
\bibitem{ref:edmonds}
	A. R. Edmonds,
	\textit{Angular Momentum in Quantum Mechanics}
\bibitem{ref:tmpchem}
	TMP Chem,
	\url{www.youtube.com/channel/UC3dZQdfv67X49cZkoXWYSwQ}
\bibitem{ref:psi4numpy}
	Psi4NumPy,
	\url{www.github.com/psi4/psi4numpy}
\end{thebibliography}

\begin{appendices}
\section{Hermite polynomials} \label{apx:hermite}
\section{Derivation of GTO integrals} \label{apx:integrals}
\end{appendices}

\end{document}

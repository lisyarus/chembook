\documentclass{article}

% for dots in table of contents
\usepackage{tocloft}
\renewcommand{\cftsecdotsep}{\cftdotsep}

% for clickable table of contents
\usepackage{hyperref}
\hypersetup{
	colorlinks = true,
	linkcolor = blue,
	citecolor = magenta,
}

% for appendices in table of contents
\usepackage[titletoc]{appendix}

% for references section in table of contents
\usepackage[nottoc]{tocbibind}

\usepackage[a4paper,margin=1in]{geometry}

\newcommand{\molecule}[1]{\textrm{#1}}

% for nice monospaced font using \texttt
\usepackage{courier}

\usepackage{enumitem}

\usepackage{titlesec}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{extarrows}

\usepackage{bashful}

\usepackage{graphicx}
\graphicspath{ {./images/book1/} }

\usepackage{minted}
% \usemintedstyle{fruity}
\definecolor{codebgcolor}{HTML}{282828}
\newenvironment{python}
{\VerbatimEnvironment
\begin{minted}[bgcolor=codebgcolor, style=fruity]{python}}
{\end{minted}}
\newenvironment{console}
{\VerbatimEnvironment
\begin{minted}[frame=single]{text}}
{\end{minted}}

\newcommand{\githubrepo}{https://github.com/lisyarus/chembook}
\newcommand{\codeonline}[1]{\textit{The final source code for this section is available \href{\githubrepo/blob/master/code/#1}{online}.}}

\newcommand{\equalby}[1]{\xlongequal{\text{by }\eqref{#1}}}
\newcommand{\intR}{\int\limits_\mathbb R}

\newcommand{\image}[1]{
\begin{center}
\includegraphics[width=\textwidth/2]{#1.png}
\end{center}
}

\setlength{\parskip}{0.4em}

\begin{document}

\title{Quantum Chemistry Done Wrong}
\author{Nikita Lisitsa \\ \texttt{lisyarus@gmail.com}}
\date{\today}

\maketitle

\bash
git rev-list --all --count
\END
\centerline{Revision \bashStdout}

\bash
[[ "$(git diff --stat)" == "" ]] || echo "(dirty)"
\END
\centerline{\bashStdout}

\newpage

\tableofcontents

\newpage

\section{What it is all about}

It was already a month of me trying to come up with a perfect introduction for this book when I decided I better do something more practical. Let's get straight to the point: what is this book about? Or, rather, what is it \textit{not} about? This is \textit{not} a

\begin{itemize}
\item Chemistry textbook
\item Quantum mechanics textbook
\item Linear algebra textbook
\item Python tutorial
\end{itemize}

But what it \textit{is}, then? Well, a sturdy mix of all the above, aimed at getting physically significant results as soon as possible, yet without sacrificing understanding. We are going to write some Python code using \href{https://numpy.org}{\texttt{numpy}} and \href{https://www.scipy.org}{\texttt{scipy}} libraries (and ocasionally \href{https://matplotlib.org}{\texttt{matplotlib}}, if you want some fancy plots \& images) and obtain energies and orbital densities for some pretty tiny chemical systems (that is, systems composed of nuclei \& electrons). You will also most probably need a file called \texttt{hgto.py} which you can download directly from \href{\githubrepo/blob/master/code/hgto.py}{my github}.

The reader is expected to have \textit{at least} a bit of knowledge on physics/chemistry, basic mathematics and Python programming language. For getting into the physics and maths involved, I recommend either reading the first few chapters of \cite{ref:atkins} or at least not skipping the \nameref{sec:var} section. For learning Python, I recommend googling some tutorials/courses/etc, for there are way too many of them to recommend anything in particular.

The following few sections briefly discuss some essential theoretical background, starting with basic mechanics. I'd especially recommend reading the \nameref{sec:var} section so that you understand what all those matrices even mean and what those solvers are actually solving. The actual coding starts at the \nameref{sec:harmonic} section.

Beware that I am by no means a scientist, a quantum chemist or even a physicist. I am barely an amateur possessing some unconventional hobbies. The methods that we are going to make use of are \textbf{not} the methods used by real quantum chemical software\footnote{This largely explains the title of the book.} and, as far as I understand, are only applicable to very tiny systems (with, say, 4 or 5 electrons, while systems of practical interest can contain \textit{thousands} of them). This book only covers one-electron systems for the reason of having a sufficiently narrowed scope.

Finally, feel free to \href{mailto:lisyarus@gmail.com}{email me} if you have some questions, I would love to be of help. Oh, and \href{https://twitter.com/lisyarus}{follow me} on twitter, I post weird simulations/computations/etc sometimes.

\bigbreak
\textit{This work was notably inspired by Peter Shirley's \href{https://raytracing.github.io}{Ray Tracing in One Weekend} series, which is an amazing source for learning about modern photorealistic rendering.}

\newpage

\section{A tale of energies}

So, what we are going to do is compute the lowest-energy states of atoms \& ions. Why is energy so important?

In any decent physical framework (e.g. classical or quantum mechanics) total energy of a closed system is conserved. It consists of several parts (e.g. kinetic + potential) which don't necessarily remain constant by themselves, but conservation of total energy implies that decrease in one part leads to increase in another. Consider a perfectly reasonable situation of a rock falling from sky: the higher it is, the higher is the potential energy; as the rock falls down, the potential energy decreases, leading to increase in kinetic energy, i.e. acceleration (some people call this \textit{gravity}).

We know that even if the rock was thrown in up the air, it will eventually fall down anyway. This is how all mechanical systems behave: they accelerate towards areas of less potential energy (this is Newton's second law). This doesn't generally mean that the system eventually falls into a local minimum of potential energy. Remember that as potential energy decreases, the kinetic energy increases, so upon reaching a local minimum the system would run with insane speeds and miss the minimum completely. This doesn't happen to rocks, though, since energy is converted into stresses in the ground when rock finally reachs it, but this does happen to pendulums: the potential energy minimum is when the pendulum is at its lowest point, yet the pendulum flies through it at high speed over and over again, never stopping there.

A system can, however, reach a local minimum of potential energy if it has some energy sink that can absorb excess kinetic energy. Typically, this sink is just the environment: a particle constantly hitting another particles quickly loses spare kinetic energy and reachs so-called thermal equilibrium. Recall that temperature is simply average kinetic energy; thus, the lower the surrounding temperature, the higher are the chances the the system will be found in a local minimum of potential energy. And \textit{this} effect is the major reason why we seek for lowest-energy states: most molecules in real life are found in those states supplemented with small kinetic fluctuations (real molecules aren't static, they wiggle and bend randomly).

Interactions can likewise be described in terms of energy: attractive force between particles means that the closer they are, the lower the energy, and repulsive force means the opposite. A \textit{bond} happens when the interaction between two systems (say, particles) has a local minimum of potential energy at some specific distance between the systems. The energy of the bond is the difference between the energy of two systems combined when the bonding takes place and the energy when they are so far that no interaction is present at all. Thus, creating a bond releases energy, while breaking it requires energy. This, coupled with some termodynamics, leads to equations that allow us to predict temperature changes in chemical reactions, provided we know the energies of the bonds. By the way, a chemical reaction is a process of forming and breaking chemical bonds. Bonds are called chemical is they happen between atomic nuclei and electrons, and usually involve at least three participants (two nuclei and an electron holding those together). As an extreme example, a piece of metal can be considered to be a single bond between all particles involved (this is so-called \textit{metallic bonding}).

I think I have convinced the reader that energies are of great utility and importance. They allow us to predict which reactions can happen and which cannot, and at what temperature. They can let us visualize how molecules look like, i.e. what is the geometry of the molecules. What is especially nice is that chemists can actually observe the energies directly, --- or, rather, the differences between them, --- simply by looking at the molecules at some special conditions: this is called \textit{spectroscopy}. Well, okay, the word ``simply'' was slightly unfair, but the point is that there are methods that can verify energy calculations with great precision.

\newpage

\section{A classical failure}

One of the first things I tried to implement when I started feeling sufficiently self-confident about my skills in 3D graphics and simulations was an \(\molecule{H}_2\) molecule. I failed miserably: most configurations were disastrous, with a few exceptions of unstable equilibria. There was no bonding, no stability, no \textit{molecule} being formed.

It turns out I've rediscovered one of the many problems of XIX-century physics: it fails to explain how atoms \& molecules work. There are many reasons for that, a few well-known chemistry-related examples being

\begin{itemize}
\item \href{https://en.wikipedia.org/wiki/Earnshaw's_theorem}{Earnshaw's theorem}: a system of charged particles cannot reach stable equilibrium by means of electrostatic forces alone, --- this is exactly what I've faced when trying to simulate those
\item Electromagnetic radiation: an electron orbiting a nucleus would lose energy by \href{https://en.wikipedia.org/wiki/Larmor_formula}{emitting electromagnetic waves}, thus eventually falling on the nucleus
\item Discrete emission spectrum: it was observed via spectroscopy that the possible energy levels of hydrogen are \href{https://en.wikipedia.org/wiki/Hydrogen_spectral_series}{discrete}, although classical mechanics always predicts a continuous range of possible energy levels
\end{itemize}

Quantum mechanics came to the rescue. The advantage of this new theory is that it correctly describes an enormous number of previously unexplained phenomena. The disadvantage, however, is that the mathematics involed is quite hard, sometimes ridiculously hard. In classical mechanics, to predict the motion of a particle, you would solve an \textit{ordinary differential equation} --- pick up your favourite method (4th-order Runge-Kutta seemingly being the most popular one) and you are done. In quantum mechanics, you have to solve a \textit{hyperbolic partial differential equation}, --- which are notoriously hard to solve numerically, --- not to mention that we don't have the notions of exact particle's posisition or momentum anymore. In classical mechanics, if you have \(N\) particles, you'd just solve \(N\) coupled ordinary differential equations using the same methods --- they work here as well. In quantum mechanics, if you have \(N\) particles, you still have a single hyperbolic partial differential equation, but in \textit{\(3N\)-dimensional space}! All conventional space discretization schemes are useless here: if you wanted to discretize each spatial direction into, say, \(10\) nodes, -- which is still too few to be useful, --- you'd need \(10^{3N}\) nodes overall. If \(N\) is the number of electrons in a water molecule, that is, \(N=10\), we get a nice value of \(10^{30}\) nodes. In comparison, your computer probably has something about \(10^{10}\) bytes of RAM.

Gladly, being difficult is very far from being impossible, and we are going to employ this nice observation.

\textit{N.B. The theory of relativity plays an important role as well, though in chemistry it is mostly responsible for much finer effects. It also limits the applicability of non-relativistic quantum mechanics to atoms with atomic number being 137 and higher (as of today, only 118 have been identified as existing), and I've written a \href{https://twitter.com/lisyarus/status/1189227697605660673?s=20}{twitter thread} on the details of this.}

\newpage

\section{Quantum mechanics: quick and dirty} \label{sec:qm}

\textit{From now on, we'll start being more technical.}

The reader will learn from any inroductory material on quantum mechanics (e.g. \cite{ref:atkins}) that in Schrödinger representation everything boils down to the \textit{time-independent Schrödinger equation}:

\begin{equation} \label{eq:shrod} \hat H \psi = E \psi \end{equation}

Physicists and chemists use the \(\widehat{hat}\) to distinguish operators \(\hat H\) from numbers \(E\). This number \(E\) is the energy of the system. It isn't always well-defined (there may be some probabilities involved), but we are interested in cases when it is. \(\psi\) is the \textit{wavefunction}, which encodes everything about the state of our physical system (though, one state corresponds to many wavefunctions, so this encoding is not unique). Technically, it is a vector from some Hilbert space, but in many real situations this Hilbert space consists of functions, so \(\psi\) is just a function, like \(\psi(x) = \sin(3x)\). The unknowns of this equation are usually both \(\psi\) and \(E\).

In quantum mechanics, every measurable physical property is encoded as a linear operator. Energy is no exception: \(\hat H\) is the energy operator, also called \textit{the Hamiltonian} (in honour of William Rowan Hamilton, who has nothing to do with quantum mechanics\footnote{He did however make significant contribution to \textit{classical} mechanics, formulating the so-called \textit{Hamiltonian formalism}, where the Hamiltonian is the energy function that corresponds directly to our operator \(\hat H\).}, in accordance with the Arnold Principle, and who also discovered quaternions). The energy operator is arguably the most important one: after all, it governs the evolution of the system (through \textit{time-dependent Schrödinger equation}, which we don't need here). All we need to know to start reasoning about a system is the energy operator (and the Hilbert space it operates on, though purists claim this piece of data is built into the operator itself, and I tend to agree). In our cases, this operator applies to functions, usually by multiplying them by other functions, or differentiating them, or both, e.g. \(\hat H \psi = -\frac{1}{2} \frac{d^2}{dx^2}\psi\). Out or pure curiosity, let us apply this operator to the \(\psi\) from previous paragraph:

\begin{equation} \hat H \psi = -\frac{1}{2} \frac{d^2}{dx^2} \sin(3x) = \frac{9}{2} \sin(3x) \end{equation}

because \(\frac{d^2}{dx^2} \sin(kx) = -k^2 \sin(kx)\). We can see that for such \(\hat H\), \(\psi\) is a solution of \eqref{eq:shrod}. Such sine-like wavefunctions are called \textit{plane waves}, while the physical system described by the operator \(\hat H\) is called a \textit{free one-dimensional particle}. In this cases, we call \(\psi\) the \textit{eigenfunction} or \textit{eigenvector} of the operator \(\hat H\), and \(E\) is the corresponding \textit{eigenvalue}.

Let us repeat:
\begin{itemize}
\item A system (say, a water molecule) is described by an operator \(\hat H\)
\item A specific state of a system (a water molecule at that point, flying in that direction with that speed) is described by a wavefunction \(\psi\) (we'll ocasionally use other letters for it, but always greek ones)
\item We seek for solutions of the equation \(\hat H \psi = E \psi\), where \(\hat H\) is known, while both \(E\) and \(\psi\) are unknowns. 
\end{itemize}

There are, in general, many solutions to this equation. It so happens in physical systems that the possible values of \(E\) are bounded below (see \href{https://www.themathcitadel.com/energy-levels-of-molecules-are-bounded-below/}{here} for a proof in case of chemical systems). If the minimal \(E\) is attained by some wavefunction \(\psi\), we call this state the \textit{ground state}. It is the state of most importance, since in general it is the most likely state a random system will be found in (the lower the temperature, the more the probability of ground state, see \href{https://en.wikipedia.org/wiki/Boltzmann_distribution}{Gibbs distribution}). It is this state that we are interested in the most.

\textit{N.B. There are severe technical difficulties in the actual mathematical formulation of quantum mechanics. They can be overcome, but the mathematics involved is far beyond the scope of this book. You can find a reasonably graceful introduction to the matter in \cite{ref:hall}.}

\newpage

\section{Variational method} \label{sec:var}

Let's recap what we've got so far: we have some \(\hat H\), and we seek for \(\psi\) that has the minimal possible value of \(E\). How can we achieve this? It is the time to lose our very last hope and accept the inevitable: there is no good way. If the Hilbert space we are working with was finite-dimensional, this would be an ordinary (yet by no means trivial!) problem of numerical linear algebra. Our Hilbert space is the space of some functions or combinations thereof, and it is \textit{infinite-dimensional}. Oops.

The trick is to reformulate our problem. First of all, however, we should talk about Hilbert spaces a bit more. They are not just vector spaces, they come with a special operation called the \textit{inner product}. If you are familiar with the dot product of vectors in geometry, this is essentially the same. Take any two functions --- say, \(\psi\) and \(\phi\), --- and you can get their inner product, denoted by \(\langle \psi, \phi \rangle\) or, using Dirac notation\footnote{Dirac notation is not used in this book.}, \(\langle \psi | \phi \rangle\). This is just a number that can be used to measure lengths, angles, etc, --- even in infinite-dimensional spaces. The definition that we are going to use (which is a pretty standard one) is the \textit{\(L^2\) inner product}, which is the integral of one function times the complex conjugate\footnote{If you are afraid of complex numbers, do not let fear stop you: we won't actually need complex numbers in this book.} of the other, i.e.

\begin{equation} \label{eq:l2prod} \langle \psi, \phi \rangle = \int \psi(x) \overline{\phi(x)} dx \end{equation}

It so happens\textsuperscript{\cite{ref:atkins}} that solutions to \(\hat H \psi = E \psi\) are exactly the stationary points of the functional\footnote{A functional is a function that applies to functions and outputs a number.}\footnote{See also: \href{https://en.wikipedia.org/wiki/Min-max_theorem\#Self-adjoint_operators}{Courant-Fischer-Weyl min-max principle}}

\begin{equation} \label{eq:rayleigh} R(\psi) = \frac{\langle \psi, \hat H \psi \rangle}{\langle \psi, \psi \rangle} \end{equation}

called the \href{https://en.wikipedia.org/wiki/Rayleigh_quotient}{Rayleigh quotient}. In particular, the value of this functional on \textit{any} wavefunction \(\psi\) is \textit{not less} than the minimal possible value of \(E\). Thus, we have our first simplest method to solve the equation: pick any random \(\psi\) that you like, plug it into \(R\), and the value you get is an upper bound to the true ground state energy. Hurray!

Of course, this is still too simple to be useful, yet it provides a good foundation. Instead of picking one \(\psi\), we could parametrize it with some parameter \(c\) so that the value of \(R\) depends on \(c\) as well and we can find the minimum of \(R\) with respect to \(c\) using techniques from undergraduate analysis!\textsuperscript{\cite{ref:atkins}} We could even take multiple \(c\)'s and minimize a function of many arguments. This is what is called the \textit{variational method} --- it works by \textit{varying} the wavefunction with respect to parameters and finding the minimum of \(R\), and can be used to get some valuable analytical estimates for various systems\textsuperscript{\cite{ref:atkins}}.

It is not that simple to minimize a function of many parameters, though, unless it depends on the parameters in some primitive way. Say, pick some functions \(\phi_1, \phi_2, \dots, \phi_k\) and let \(\psi(x) = c_1 \phi_1(x) + \dots + c_k \phi_k(x) \). This is the idea of \textit{linear variational method}\footnote{Also known as the \textit{Rayleigh-Ritz method}}, where the dependence of \(\psi\) on the parameters is, well, linear. After taking a few derivatives and doing a bit of algebra one arrives\textsuperscript{\cite{ref:atkins}} at this matrix equation:

\begin{equation} \label{eq:var} H c = E S c \end{equation}

where

\begin{itemize}
\item \(H\) is an \(k\times k\)-matrix with entries \(H_{ij} = \langle \phi_i, \hat H \phi_j \rangle = \langle \hat H \phi_i, \phi_j \rangle\) (the second equality is true because \(\hat H\) is \textit{self-adjoint}).
\item \(S\) is an \(k\times k\)-matrix (called the \textit{overlap matrix}\footnote{In mathematics, this one is better known as \href{https://en.wikipedia.org/wiki/Gramian_matrix}{Gramian matrix}.}) with entries \(S_{ij} = \langle \phi_i, \phi_j \rangle\)
\item \(c\) is the column vector of parameters \(c_1, \dots, c_k\)
\end{itemize}

This is known as a \textit{generalized eigenvalue problem} (as opposed to the regular \textit{eigenvalue problem} that lacks \(S\)), and it is \textbf{this} equation that we will try to solve.

\section{Harmonic oscillator} \label{sec:harmonic}

Before we get into chemiscal systems, atomic nuclei and electrons, let's first try our hand at a simpler toy quantum system --- the one-dimensional \href{https://en.wikipedia.org/wiki/Quantum_harmonic_oscillator}{\textit{harmonic oscillator}}. It works roughly like a pendulum, being forced back the more it moves off the center\footnote{Any system that is sufficiently close to a local potential energy minimum behaves like a harmonic oscillator. The reason is that in a minimum the derivative of potential energy is zero, so it is well-approximated by the quadratic term: \(U(x+\delta)=U(x)+\frac{1}{2}U''(x)\delta^2\).}. The Hilbert space for this system is denoted \(L^2(\mathbb R)\) --- the space of functions of one real argument that have a finite integral of their modulus squared (in other words, \(\langle \psi, \psi \rangle\) is finite). We should be working with complex-valued functions, but as I'll explain in the next section, we can actually restrict ourselves to real numbers everywere.

The Hamiltonian for this system looks like this:

\begin{equation} \label{eq:harmonic-hamiltonian} \hat H \psi = -\frac{1}{2} \frac{d^2}{dx^2} \psi + \frac{x^2}{2} \psi \end{equation}

And the energy eigenvalues are\cite{ref:atkins}

\begin{equation} E_k = k + \frac{1}{2}, k = 0,1,2,\dots \end{equation}

Thus, the ground state energy is \(E_0=\frac{1}{2}\). Let's try to get this number without solving the equation \eqref{eq:shrod} directly, employing the equation \eqref{eq:var} instead!

All we need to do is pick a good selection of \textit{basis functions} \(\phi_k\), also called the \textit{basis set}. What makes a basis set better or worse? We have a dilemma here: on one hand, the basis should be somewhat resembling the actual ground state, so that we can get closer to real ground state energy; on the other hand, we'll have to compute a bunch of integrals \(\langle \phi_i, \phi_j\rangle\) and \(\langle \phi_i, \hat H \phi_j\rangle\), so the basis functions should be simple enough so that the integrals are tractable\footnote{We could approach them numerically, but this is orders of magnitude slower and less accurate.}.

There are many options here, and I encourage you to try your own basis set if you are not afraid of computing a few integrals. The one I'll use will be

\begin{equation} \phi_a(x) = e^{-(x-a)^2} \end{equation}

This \textit{gaussian} functions are sufficiently localized and easy to integrate at the same time, which makes them a good choice. The parameter \(a\) can be any real number here.

First of all, we need to compute those integrals. You may skip the derivations and go straight to the code, since we will only need the final formulas. The overlap integral is the simplest:

\begin{equation} \label{eq:exp-overlap}
\begin{gathered}
\langle \phi_a, \phi_b \rangle = \int\limits_\mathbb{R} \phi_a(x) \phi_b(x) dx = \int\limits_\mathbb{R} e^{-(x-a)^2} e^{-(x-b)^2} dx = \int\limits_\mathbb{R} e^{-2x^2+2(a+b)x-\left(a^2+b^2\right)} dx =
\\
= \int\limits_\mathbb{R} e^{-2\left(x^2-2\frac{a+b}{2}x + \left(\frac{a+b}{2}\right)^2\right)} e^{\frac{(a+b)^2}{2} - \left(a^2+b^2\right)} dx = \int\limits_\mathbb{R} e^{-2\left(x-\frac{a+b}{2}\right)^2} e^{-\frac{(a-b)^2}{2}} dx = \sqrt{\frac{\pi}{2}} e^{-\frac{(a-b)^2}{2}}
\end{gathered}
\end{equation}

We have used the following well-known \href{https://en.wikipedia.org/wiki/Gaussian_integral}{Gaussian integral}:

\begin{equation} \label{eq:gauss}
\intR e^{-p(x-a)^2} dx = \sqrt{\frac{\pi}{p}}
\end{equation}

The equality

\begin{equation} \label{eq:gauss-product}
e^{-(x-a)^2} e^{-(x-b)^2} = e^{-2\left(x-\frac{a+b}{2}\right)^2} e^{-\frac{(a-b)^2}{2}}
\end{equation}

is occasionally known as the \href{https://en.wikipedia.org/wiki/Gaussian_orbital#Rationale}{Gaussian product theorem} and is super useful for integrals with several gaussians.

Next, we need a few handy observations:

\begin{equation} \label{eq:dexp}
\frac{d}{dx} e^{-p(x-a)^2} = -2p(x-a)e^{-p(x-a)^2}
\end{equation}

\begin{equation} \label{eq:int-dexp-is-zero}
\intR (x-a)e^{-p(x-a)^2} \equalby{eq:dexp} -\frac{1}{2p} \intR \left[\frac{d}{dx} e^{-p(x-a)^2}\right] dx = \left. -\frac{1}{2p} e^{-p(x-a)^2}\right\vert_{-\infty}^{+\infty} = 0
\end{equation}

(by the fundamental theorem of calculus).

\begin{equation} \label{eq:int-x-exp}
\begin{gathered}
\intR x e^{-p(x-a)^2} dx = \intR \left[ (x-a)+a\right] e^{-p(x-a)^2} dx = \intR (x-a)e^{-p(x-a)^2} dx + a \intR e^{-p(x-a)^2} dx = \\
\equalby{eq:int-dexp-is-zero} 0 + a \intR e^{-p(x-a)^2} dx \equalby{eq:gauss} a \sqrt\frac{\pi}{p}
\end{gathered}
\end{equation}

\begin{equation} \label{eq:ddexp}
\frac{d^2}{dx^2} e^{-p(x-a)^2} \equalby{eq:dexp} \frac{d}{dx} \left[ -2p(x-a)e^{-p(x-a)^2} \right] = \left[-2p + 4p^2(x-a)^2\right] e^{-p(x-a)^2}
\end{equation}

\begin{equation} \label{eq:int-xx-exp}
\begin{gathered}
\intR (x-a)^2 e^{-p(x-a)^2} dx = \frac{1}{4p^2} \intR 4p^2 (x-a)^2 e^{-p(x-a)^2} dx = \\
= \frac{1}{4p^2} \intR \left[ 4p^2 (x-a)^2 - 2p \right] e^{-p(x-a)^2} dx + \frac{1}{2p} \intR e^{-p(x-a)^2} dx \equalby{eq:ddexp} \\
= \frac{1}{4p^2} \intR \left[\frac{d^2}{dx^2} e^{-p(x-a)^2}\right]dx + \frac{1}{2p} \sqrt\frac{\pi}{p} = \left. \frac{1}{4p^2} \left[\frac{d}{dx} e^{-p(x-a)^2}\right] \right\vert_{-\infty}^{+\infty} + \frac{1}{2p}\sqrt\frac{\pi}{p} = \\
= 0 + \frac{1}{2p}\sqrt\frac{\pi}{p} = \sqrt\frac{\pi}{4p^3}
\end{gathered}
\end{equation}

(again, using the fundamental theorem of calculus).

Now we are ready for the energy integral \(\langle \phi_a, \hat H\phi_b\rangle\). We will split it into kinetic \(-\frac{1}{2}\frac{d^2}{dx^2}\) and potential \(\frac{1}{2}x^2\) parts (see the definition\eqref{eq:harmonic-hamiltonian} of \(\hat H\) for harmonic oscillator).

\begin{equation} \label{eq:exp-potential}
\begin{gathered}
\left\langle \phi_a , \frac{1}{2} x^2 \phi_b \right\rangle = \intR e^{-(x-a)^2} \frac{1}{2} x^2 e^{-(x-b)^2} dx \equalby{eq:gauss-product} \frac{1}{2} e^{-\frac{(a-b)^2}{2}} \intR x^2 e^{-2\left(x-\frac{a+b}{2}\right)^2} dx = \\
= \frac{1}{2} e^{-\frac{(a-b)^2}{2}} \intR \left[\left( x-\frac{a+b}{2} \right)^2 + x(a+b) - \frac{(a+b)^2}{4} \right] e^{-2\left(x-\frac{a+b}{2}\right)^2} dx \equalby{eq:int-xx-exp} \\
= \frac{1}{2} e^{-\frac{(a-b)^2}{2}} \left[ \sqrt\frac{\pi}{32} + \intR \left[ x(a+b) - \frac{(a+b)^2}{4} \right] e^{-2\left(x-\frac{a+b}{2}\right)^2} dx \right] \equalby{eq:int-x-exp} \\
= \frac{1}{2} e^{-\frac{(a-b)^2}{2}} \left[ \sqrt\frac{\pi}{32} + \frac{(a+b)^2}{2}\sqrt\frac{\pi}{2} - \frac{(a+b)^2}{4} \intR e^{-2\left(x-\frac{a+b}{2}\right)^2} dx \right] \equalby{eq:gauss} \\
= \frac{1}{2} e^{-\frac{(a-b)^2}{2}} \left[ \sqrt\frac{\pi}{32} + \frac{(a+b)^2}{2}\sqrt\frac{\pi}{2} - \frac{(a+b)^2}{4} \sqrt\frac{\pi}{2} \right] = \\
= \frac{1}{2} e^{-\frac{(a-b)^2}{2}} \sqrt\frac{\pi}{2} \left[ \frac{1}{4} + \frac{(a+b)^2}{4} \right] \equalby{eq:exp-overlap} \frac{1+(a+b)^2}{8} \langle \phi_a, \phi_b \rangle
\end{gathered}
\end{equation}

Finally, the kinetic integral:

\begin{equation} \label{eq:exp-kinetic}
\begin{gathered}
\left\langle \phi_a , -\frac{1}{2} \frac{d^2}{dx^2} \phi_b \right\rangle = -\intR e^{-(x-a)^2} \frac{1}{2} \left[ \frac{d^2}{dx^2} e^{-(x-b)^2} \right] dx \equalby{eq:ddexp} \\
= -\intR \left[ 2(x-b)^2 -1 \right] e^{-(x-a)^2} e^{-(x-b)^2} dx = \\
= -\intR \left[ 2x^2 - 4bx + 2b^2-1 \right] e^{-(x-a)^2} e^{-(x-b)^2} dx = \\
= - 4 \intR e^{-(x-a)^2} \frac{1}{2} x^2 e^{-(x-b)^2} dx + 4b \intR x e^{-(x-a)^2} e^{-(x-b)^2} dx - (2b^2-1) \intR e^{-(x-a)^2} e^{-(x-b)^2} dx = \\
= - 4 \left\langle \phi_a, \frac{1}{2} x^2 \phi_b \right\rangle + 4b \intR x e^{-(x-a)^2} e^{-(x-b)^2} dx - (2b^2-1) \langle \phi_a, \phi_b \rangle \equalby{eq:exp-potential} \\
= - \frac{1+(a+b)^2}{2} \langle \phi_a, \phi_b \rangle + 4b \intR x e^{-(x-a)^2} e^{-(x-b)^2} dx - (2b^2-1) \langle \phi_a, \phi_b \rangle = \\
= - \left[\frac{1+(a+b)^2}{2} + 2b^2 - 1\right] \langle \phi_a, \phi_b \rangle + 4b \intR x e^{-(x-a)^2} e^{-(x-b)^2} dx \equalby{eq:gauss-product} \\
= - \left[\frac{1+(a+b)^2}{2} + 2b^2 - 1\right] \langle \phi_a, \phi_b \rangle + 4b e^{-\frac{(a-b)^2}{2}} \intR x e^{-2\left(x-\frac{a+b}{2}\right)^2} dx \equalby{eq:int-x-exp} \\
= - \left[\frac{1+(a+b)^2}{2} + 2b^2 - 1\right] \langle \phi_a, \phi_b \rangle + 4b e^{-\frac{(a-b)^2}{2}} \frac{a+b}{2} \sqrt\frac{\pi}{2} \equalby{eq:exp-overlap} \\
= - \left[\frac{1+(a+b)^2}{2} + 2b^2 - 1 -4b\frac{a+b}{2}\right] \langle \phi_a, \phi_b \rangle = \\
= - \frac{1}{2} \left[ 1 + (a+b)^2 + 4b^2 - 2 - 4b(a+b)\right] \langle \phi_a, \phi_b \rangle = \\
= \frac{1 - (a-b)^2}{2} \langle \phi_a, \phi_b \rangle 
\end{gathered}
\end{equation}

We could as well put this into \href{https://www.wolframalpha.com/}{WolframAlpha} and get the results immediately. Anyways, we are ready to code!

\codeonline{harmonic.py}

First of all, import the required libraries:

\begin{python}
import math
import numpy
import scipy.linalg
\end{python}

Write down the formulas for overlap\eqref{eq:exp-overlap}, kinetic\eqref{eq:exp-kinetic}, and potential\eqref{eq:exp-potential} integrals in python code:

\begin{python}
def overlap(a, b):
    return math.sqrt(math.pi / 2) * numpy.exp(- (a - b)**2 / 2)


def potential(a, b):
    return ((a + b)**2 + 1) / 8 * overlap(a,b)


def kinetic(a, b):
    return (1 - (a-b)**2) / 2 * overlap(a,b)
\end{python}

I am using \texttt{numpy.exp} instead of \texttt{math.exp} so that we can pass numpy arrays in place of \texttt{a} and \texttt{b}.

Now, time to build the matrices of our main equation\eqref{eq:var}: \(H \psi = E S \psi\). We need a function that, given a description of a basis set, returns a pair of matrices \texttt{(H,S)}. Our basis functions \(\psi_a\) are parametrized by a single parameter \texttt{a}, so a basis set can be described as a list of numbers that correspond to different \texttt{a}'s.

\begin{python}
def equation(basis):
    D = len(basis)
    H = numpy.zeros((D,D))
    S = numpy.zeros((D,D))

    for i in range(D):
        for j in range(D):
            ai = basis[i]
            aj = basis[j]

            H[i,j] = kinetic(ai,aj) + potential(ai,aj)
            S[i,j] = overlap(ai,aj)

    return (H, S)
\end{python}

Next, we need a function that will solve the generalized eigenvalue problem\eqref{eq:var}. Luckily, \texttt{scipy} package already knows how to deal with these! We also need to normalize the resulting wavefunctions, but \texttt{scipy.linalg.eigh} does this as well.

\begin{python}
def solve(H, S):
    return scipy.linalg.eigh(H, S)
\end{python}

We have all the building blocks to run the code. What basis shall we pick, though? Let's try something silly, like a basis of \(e^{-(x+2)^2}\), \(e^{-(x+1)^2
}\), \(e^{-x^2}\), \(e^{-(x-1)^2}\), and \(e^{-(x-2)^2}\):

\begin{python}
basis = [-2, -1, 0, 1, 2]
energy, coeffs = solve(*equation(basis))
print("Energy:", energy)
\end{python}

Here, \texttt{energy} is an array of energy eigenvalues (values of \(E\) in \(H\psi=ES\psi\)) while \texttt{coeffs} is a matrix with its k-th \textit{column} containing the coefficients \(c_i\) in the sum \(\psi(x) = \sum c_i \phi_i(x)\) for the quantum state \(\psi\) that corresponds to the k-th value of \(E\).

If you run the script, you should get something like

\begin{console}
Energy: [0.50028959 1.50654872 2.50906592 3.65198693 4.52712922]
\end{console}

We got the ground-state energy of \texttt{0.50028959}, correct within 0.06\%! Even the next two energies are very close to the real values of \texttt{1.5} and \texttt{2.5}. Congratulations! You've just completed your (probably) first numerical quantum-mechanical computation. Before we move on to chemical systems, let's see what else we can do with our current code.

We can optimize the \texttt{equation} function by using \texttt{numpy.meshgrid}, since operations on \texttt{numpy} arrays are generally faster than python loops:

\begin{python}
def equation(basis):
    Ai, Aj = numpy.meshgrid(basis, basis)

    H = kinetic(Ai, Aj) + potential(Ai, Aj)
    S = overlap(Ai, Aj)

    return (H, S)
\end{python}

We can plot the wavefunction that we've found and compare it against the true ground-state wavefunction \(\frac{1}{\sqrt[4]{\pi}}e^{-\frac{x^2}{2}}\). For that matter, define a function that computes \(\phi_a\):

\begin{python}
def phi(a, x):
	return numpy.exp(-(x-a)**2)
\end{python}

Then, a function that evaluates the wavefunction:

\begin{python}
def wavefunction(basis, coeffs, x):
    assert len(basis) == len(coeffs)

    y = numpy.zeros(x.shape)
    for i in range(len(basis)):
        y += coeffs[i] * phi(basis[i], x) 
    return y
\end{python}

For convenience, let's define the real wavefunction as well:

\begin{python}
def ground_state(x):
	return 1.0 / math.pow(math.pi, 0.25) * numpy.exp(- x**2 / 2.0)
\end{python}

To plot the results, add this to the end of the script:

\begin{python}
x = numpy.linspace(-5, 5, 1001)
computed_gound_state = wavefunction(basis, coeffs[:,0], x)
real_ground_state = ground_state(x)

pyplot.plot(x, computed_gound_state, label="computed")
pyplot.plot(x, real_ground_state, label="real")
pyplot.grid()
pyplot.legend()
pyplot.show()
\end{python}

The \texttt{:} in \texttt{coeffs[:,0]} is a special placeholder that mean \textit{all possible indices}. Thus, \texttt{coeffs[:,0]} takes the 0-th column of \texttt{coeffs}.

If you run the code, you might (or might not!) get something like this:

\image{harmonic-wavefunction-negative}

Why is your wavefunction upside down?! There's nothing wrong with that, actually: if \(\psi\) is a solution, then \(-\psi\) is a solution as well. In fact, these two represent \textit{the same quantum state}. We could negate our wavefunction if it is negative at \(x=0\):

\begin{python}
x = numpy.linspace(-5, 5, 1001)
computed_gound_state = wavefunction(basis, coeffs[:,0], x)
real_ground_state = ground_state(x)

if wavefunction(basis, coeffs[:,0], 0.0) < 0.0:
    computed_gound_state *= -1.0

pyplot.plot(x, computed_gound_state, label="computed")
pyplot.plot(x, real_ground_state, label="real")
pyplot.grid()
pyplot.legend()
pyplot.show()
\end{python}

This code will result in an error like this:

\begin{console}
AttributeError: 'float' object has no attribute 'shape'
\end{console}

Oops, we need to fix \texttt{wavefunction} so that it works in case \texttt{x} is a number and not a \texttt{numpy} array:

\begin{python}
def zero_like(x):
    if type(x) == numpy.ndarray:
        return numpy.zeros(x.shape)
    elif type(x) == float:
        return 0.0
    else:
        raise RuntimeError("Unknown type {}".format(type(x)))


def wavefunction(basis, coeffs, x):
    assert len(basis) == len(coeffs)

    y = zero_like(x)
    for i in range(len(basis)):
        y += coeffs[i] * phi(basis[i], x) 
    return y
\end{python}

This time, we should have the two wavefunctions on the same side of the x-axis:

\image{harmonic-wavefunction}

As you can see, our computed solution is very close the the true one! Can we get any closer? Sure, but we need a better basis than just \texttt{[-2, -1, 0, 1, 2]}. We need to tweak a bunch of numbers so that another number (the energy) gets smaller --- this is precisely what is called mathematical optimization. Luckily, \texttt{scipy} already comes with routines that do just that! Add this to imports:

\begin{python}
import scipy.optimize
\end{python}

Now, define our target function that we want to minimize --- the ground-state energy:

\begin{python}
def target(basis):
	energy, coeffs = solve(*equation(basis))
	return energy[0]
\end{python}

The main script now goes as follows: we pick initial basis parameters, optimize them, and see the energy values:

\begin{python}
def optimize(basis):
	result = scipy.optimize.minimize(target, basis)
	if not result.success:
	    raise RuntimeError("Energy minimization failed")
	return result.x


basis = optimize([-2, -1, 0, 1, 2])
print("Basis:", basis)

energy, coeffs = solve(*equation(basis))
print("Energy:", energy)
\end{python}

The results are pretty convincing:

\begin{console}
Basis: [-1.62551507e+00 -7.69713217e-01 -1.08490137e-04  7.69683879e-01
  1.62560700e+00]
Energy: [0.50000139 1.50009365 2.50253397 3.53463459 4.78613676]
\end{console}

We can plot some more wavefunctions, not only the ground state:

\begin{python}
x = numpy.linspace(-5, 5, 1001)

for k in range(3):
    state = wavefunction(basis, coeffs[:,k], x)
    pyplot.plot(x, state, label="E = {}".format(energy[k]))

pyplot.grid()
pyplot.legend()
pyplot.show()
\end{python}

\image{harmonic-wavefunction-3}

We could try the same thing starting with a larger basis, say, from \(-5\) to \(5\):

\begin{python}
basis = optimize(range(-5, 6))
energy, coeffs = solve(*equation(basis))
print("Energy:", energy)
\end{python}

Notice how the ground-state energy goes even closer to the true value of \(0.5\).

\begin{console}
Energy: [ 0.50000007  1.50000621  2.5002545   3.50575575  4.56901491
  5.85678673  6.52567398  9.508999    9.70153557 14.49609236
 14.55364916]
\end{console}

This section is already 8 pages long. I hope you got the basis idea, for it is time to move on to real atoms.

\newpage

\section{Practical considerations} \label{sec:pract}

We have seen that in order to use the linear variational method we need to pick a nice basis set --- a bunch of parametrized functions that are well-suited for integrals. A particularly nice choice for chemical systems are \textit{gaussian-type orbitals}, often abbreviated as GTOs, multiplied by Hermite polynomials. Usually, \textit{cartesian} GTOs (CGTOs) are used, where a gaussian is multiplied by monomials of the form \(x^ay^bz^c\). However, Hermite polynomials make the integrals much easier, --- in fact, the derivations of integrals for CGTOs often include steps to convert them to Hermite polynomials, so why not use them directly instead? Furthemore, this cartesian gaussian-type orbitals are usually used indirectly: the actual basis set used is Slater-type orbitals (STOs), with functions taken directly from eigenstates of the hydrogen atom, but those are internally represented as a linear combination of GTOs (known as \textit{contracted} GTOs or cGTOs) to simplify the integrals. It is however a lot easier to work with gaussian-type functions instead. The general form of such a function looks like this:

\begin{equation}
\phi_{R,\sigma,n}(x,y,z) = H_{n_x}(x)H_{n_y}(y)H_{n_z}(z) e^{-\frac{(x-R_x)^2+(y-R_y)^2+(z-R_z)^2}{\sigma^2}}
\end{equation}

It has three integer parameters \(n_x,n_y,n_z\) and four real parameters \(R_x,R_y,R_z,\sigma\). This function is localized around the point \((R_x,R_y,R_z)\), with the parameter \(\sigma\) controlling the spread (less \(\sigma\) means more localized function). \(H_n(x)\) is the \(n\)-th Hermite polynomial, and the integers \(n_x,n_y,n_z\) are responsible for that. They make out basis functions look more like hydrogen p-orbitals, d-orbitals, f-orbitals, and so on. I've put the derivations of the integrals with this basis functions to the appendix. These integrals are arguably the most difficult and the least interesting or instructive part of the whole process. You may look into the appendix and type the formulas into python (or whatever language you use) yourself, or you may download a package from \href{\githubrepo/blob/master/code/hgto.py}{github} that implements them. The code in the following sections assumes the latter.

As I've already said, although complex numbers are invaluable in quantum mechanics, we don't need them. The reason is that in chemical systems the energy eigenstates can always be made real\footnote{Technically, this boils down to the fact that the Hamiltonian commutes with complex conjugation, thus also with projectors on spaces of real-valued and imaginary-valued functions. If an eigenstate is purely imaginary-valued, multiply it by \(i\) to get a real-valued one, otherwise apply the projector to real-valued functions.}. Note however that a general state is a mixture of those eigenstates with \textit{complex coefficients}.

Any real-world computation is unimaginable without a choice of units. There is a specialized unit system called the \href{https://en.wikipedia.org/wiki/Hartree_atomic_units}{\textit{atomic units}}, where most constants relevant to quantum chemistry are set to \(1\), specifically

\begin{itemize}
\item The electron mass \(m_e\)
\item The electron charge \(e\)
\item The reduced Planck constant \(\hbar\)
\item The Coulomb constant \(k_e\)
\end{itemize}

The energy unit in this system is called \href{https://en.wikipedia.org/wiki/Hartree}{\textit{hartree}} and equals approximately \(27.2\) electron-volts or \(4.35\times 10^{-18}\) joules. There's a \href{http://www.colby.edu/chemistry/PChem/Hartree.html}{nice energy converter online}. The speed of light in this unit system is just \(137\). Keep all that in mind when reading the equations in the following sections: there are no physical constants in them, not because they shouldn't be there, but because they are all equal to \(1\).

One final note: all that follows assumes the \href{https://en.wikipedia.org/wiki/Born\%E2\%80\%93Oppenheimer_approximation}{Borh-Oppenheimer approximation}, which states that since atomic nuclei are much\footnote{A proton is 1836 times more massive than an electron, see \href{https://en.wikipedia.org/wiki/Proton-to-electron_mass_ratio}{a corresponding wikipedia article}.} heavier than the electrons, the latter should move orders of magnitude faster, thus it is possible to separate the movement of nuclei from that of electrons, effectively treating the nuclei as being static. This is exactly what we are going to do: solve the equations for the electrons assuming static nuclei, and solve for the nuclei separately.

\newpage

\section{Hydrogen atom \(\molecule{H}\)}

At last we arrived to an actual chemical problem: the hydrogen atom. Assuming that the nucleus is at the origin, the Hamiltonian looks like this:

\begin{equation}
\hat H\psi = -\frac{1}{2} \Delta \psi - \frac{1}{|r|}\psi
\end{equation}

Here, 

\begin{itemize}
\item \(\psi\) is a function (the wavefunction) of a three-component vector \(r\)\footnote{\(r\) may be thought of as representing the position of the electron, but care should be taken, for there is no \textit{the} position of an electron due to \href{https://en.wikipedia.org/wiki/Uncertainty_principle}{Heisenberg uncertainty}, and each possible \(r\) has a probability of finding the electron in that particular place.} (technically, an element of the space \(L^2(\mathbb R^3)\))
\item \(\Delta\) is the \href{https://en.wikipedia.org/wiki/Laplace_operator}{Laplace operator} \(\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2}\) and accounts for the kinetic energy of an electron
\item \(|r|=\sqrt{x^2+y^2+z^2}\) is the distance to nucleus
\item \(-\frac{1}{|r|}\) is the \href{https://en.wikipedia.org/wiki/Electric_potential_energy}{Coulomb potential energy} of the attraction between electron (at distance \(|r|\)) and hydrogen nucleus.
\end{itemize}

It is one of the few real-world quantum systems with known analytical solutions (see \href{https://en.wikipedia.org/wiki/Hydrogen-like_atom}{here}). The energies are (in atomic units!) \(E_n = -\frac{1}{2n^2}\), where \(n=1,2,3,\dots\) is that number written in front of atomic orbitals like 1s, 2s, 2p, 3s, 3p, 3d, etc. The corresponding wavefunctions look like those \href{https://en.wikipedia.org/wiki/Atomic_orbital}{funny blobs}\footnote{See also \href{https://lisyarus.github.io/webgl/gto.html}{my interactive visualizer}.} from any introductory chemistry textbook.

\codeonline{hydrogen-s.py}

As I've said in the \nameref{sec:pract} section, the code assumes you have the \texttt{hgto} module that implements all the integrals for Hermite-Gaussian orbitals. Start with the imports:

\begin{python}
import numpy
import scipy.linalg
import scipy.optimize
import hgto
\end{python}

Now, the already familiar equation-building function:

\begin{python}
def equation(basis):

    B = len(basis)

    H = numpy.zeros((B,B))
    S = numpy.zeros((B,B))

    n = hgto.Nucleus(1.0)

    for i in range(B):
        for j in range(B):
            bi = basis[i]
            bj = basis[j]
            S[i,j] = hgto.overlap(bi, bj)
            H[i,j] = hgto.kinetic(bi, bj) + hgto.nuclear_attraction(bi, bj, n)

    return (H, S)
\end{python}

You've probably noticed that we have a new thing here: the nucleus, represented as an object of type \texttt{hgto.Nucleus}. The constructor has a second parameter \texttt{origin} that corresponds to the location of the nucleus. In the case of a single atom, it is reasonable to leave it at the default value of \texttt{[0.0,0.0,0.0]}. The \texttt{nuclear\_attraction} function computes the integral corresponding to attraction to that single nucleus. \texttt{basis} is a list of basis functions represented as objects of type \texttt{hgto.HGTO}.

Next, the function that solves the generalized eigenvalue equation doesn't change at all:

\begin{python}
def solve(H, S):
    return scipy.linalg.eigh(H, S)
\end{python}

Now, time to pick a set of basis functions. The constructor of the \texttt{hgto.HGTO} class looks like this:

\begin{python}
    def __init__(self, sigma, degree=[0,0,0], origin=[0.0,0.0,0.0]):
        ...
\end{python}

The \texttt{sigma}, \texttt{degree}, and \texttt{origin} parameters correspond to \(\sigma\), \([n_x,n_y,n_z]\), and \([R_x,R_y,R_z]\) parameters of an HGTO-orbital \(H_{n_x}(x)H_{n_y}(y)H_{n_z}(z) e^{-\frac{(x-R_x)^2+(y-R_y)^2+(z-R_z)^2}{\sigma^2}}\), respectively. Since we are working with a single atom, it is reasonable to set the origin of all basis functions to the location of the nucleus.

The \texttt{degree} parameter is a bit subtler. The value of \texttt{[0,0,0]} corresponds to a basis function that is radially symmetric, much like the s-orbitals of hydrogen, so let's start with them. Thus, our only adjustable parameters will be \texttt{sigma}. Write a function that takes a list of \texttt{sigma} and outputs a corresponding basis:

\begin{python}
def make_basis(params):
    basis = []

    for sigma in params:
        basis.append(hgto.HGTO(sigma))

    return basis
\end{python}

As before, define the optimization target function and the optimization routine. This time, we will select the optimization method other than the default one: I figured that the \textit{Nelder-Mead} simplex-based method works pretty well (you may try other methods, of course!). We'll say it to be adaptive, so that it tweaks the internal parameters as needed. It is also helpful to provide a callback function that will record optimization progress.

\begin{python}
def target(params):
    basis = make_basis(params)
    energy, coeffs = solve(*equation(basis))
    return energy[0]


def callback(params):
    print(params)


def optimize(params):
    result = scipy.optimize.minimize(target, params, method='Nelder-Mead',
        options={'adaptive': True}, callback=callback)
    if not result.success:
        raise RuntimeError("Energy minimization failed")
    return result.x
\end{python}

Finally, pick a random set of, say, five initial parameters, invoke the optimization, and look at the results. How do I know to choose the parameters in the \([0.5, 5.0]\) range? This is where our choice of units helps: all related constants are set to 1, and we can expect all physically significant properties of the system to be of the same order of magnitude, that is, close to 1. Thus, we choose some range that is close to 1.

\begin{python}
params = optimize(numpy.random.uniform(0.5, 5.0, 5))

basis = make_basis(params)
energy, coeffs = solve(*equation(basis))
print("Energy:", list(energy))
\end{python}

You might see some errors here. Firstly, the minimization routine might not succeed after some default maximal number of iterations, --- this can probably be fixed by adjusting the solver options. Secondly, you may encounter an error like this:

\begin{console}
numpy.linalg.LinAlgError: the leading minor of order 2 of 'b' is not 
positive definite. The factorization of 'b' could not be completed and
no eigenvalues or eigenvectors were computed.
\end{console}

This comes from the Cholesky decomposition that is invoked as part of \texttt{scipy.linalg.eigh} and means that the basis that we supplied is too close to being linearly dependent. This can be fixed using singular value decomposition and restricting to a better subbasis.

It is however much easier to just call the script again in hope that this time the random initial parameters would be better, or even wrap the \texttt{optimize} routine with a loop that starts over and over again until the optimization succeeds without an exception. In any case, you should get something like this:

\begin{console}
Energy: [-0.4998098322278446, 0.023987247113695483, 1.5010945028423488,
 9.012692390104283, 60.979054059578246]
\end{console}

The ground-state energy is very close to the actual value of \(-0.5\), while the others are complete garbage (they are even positive, meaning that they do not correspond to bound states). This is a general phenomenon: is you optimize for something, don't expect something else to be optimized as well. There are several ways to make this work for excited (\(n=2,3,\dots\)) states. One could find the ground state first, then employ the same procedure but with an extra step of reprojecting the basis on the subspace orthogonal to the ground state to find the first excited state, then doing the same but orthogonal to the two already found states to find the next excited state, and so on. This makes the basis grow rapidly (since you need both the old basis and the new one to make the new one orthogonal to the old). We may, however, try to trick our solver a bit, and make it optimize for the \textit{sum} of a few lowest energies (say, three of them), at the price of having a lower overall precision:

\begin{python}
def target(params):
    basis = make_basis(params)
    energy, coeffs = solve(*equation(basis))
    return sum(energy[0:3])
\end{python}

The results are not that bad:

\begin{console}
Energy: [-0.4976228254475445, -0.12462108469029497, -0.05542836872909056,
 0.5864866278502727, 7.547798891619299]
\end{console}

The first three energy values are pretty close to the actual values of \(-0.5\), \(-0.125\), and \(-\frac{1}{18}=-0.0(5)\), corresponding to 1s, 2s, and 3s hydrogen orbitals. Time to visualize them!

How does one visualize a three-dimensional function? Usually this requires volume rendering, isosurfaces, or slicing the space. However, while we are dealing with highly symmetric functions, we can manage without such sophisticated machinery. Recall that s-orbitals are spherically symmetric, meaning that we can just plot a cross-section along the X axis.

As before, add \texttt{matplotlib} to the imports:

\begin{python}
import matplotlib.pyplot as pyplot
\end{python}

Write the function that evaluates our wavefunction at a specific point (\texttt{x}, \texttt{y}, and \texttt{z} may as well be \texttt{numpy} arrays):

\begin{python}
def zero_like(x):
    if type(x) == numpy.ndarray:
        return numpy.zeros(x.shape)
    elif type(x) == float:
        return 0.0
    else:
        raise RuntimeError("Unknown type {}".format(type(x)))


def wavefunction(basis, coeffs, x, y, z):
    assert len(basis) == len(coeffs)

    f = zero_like(x)
    for i in range(len(basis)):
        f += coeffs[i] * basis[i](x, y, z)
    return f
\end{python}

Now, evaluate our three lowest states along the X axis and plot them:

\begin{python}
L = 30
N = 1001
X = numpy.linspace(-L, L, N)

for k in range(3):
    state = wavefunction(basis, coeffs[:,k], X, 0, 0)
    pyplot.plot(x, state, label="E = {0:.5f}".format(energy[k]))

pyplot.grid()
pyplot.legend()
pyplot.show()
\end{python}

This results in something like (remember that the wavefunctions may be upside down)

\image{hydrogen-wavefunction-s-3}

A few notable things can be read from this plot:

\begin{itemize}
\item All states have singularities\footnote{Even for many-electron systems one can reconstruct the original system knowing this cusps only, see \href{https://en.wikipedia.org/wiki/Kato_theorem}{Kato's cusp condition}. This is especially important for the foundations of \href{https://en.wikipedia.org/wiki/Density_functional_theory}{density functional theory (DFT)}.} at the origin, where the nucleus is located
\item Higher-energy orbitals are less concentrated near the nucleus and are more spread around
\item 1s orbital has the same sign everywhere, 2s orbital changes sign once, 3s changes sign twice, etc.
\end{itemize}

Let's plot the electron density along the X axis as well:

\begin{python}
def density(basis, coeffs, x, y, z):
    assert len(basis) == len(coeffs)

    f = zero_like(x)
    for i in range(len(basis)):
        for j in range(len(basis)):
            f += coeffs[i] * coeffs[j] * basis[i](x, y, z) * basis[j](x, y, z)
    return f
\end{python}

\begin{python}
for k in range(3):
    state = density(basis, coeffs[:,k], X, 0, 0)
    pyplot.plot(x, state, label="E = {0:.5f}".format(energy[k]))

pyplot.grid()
pyplot.legend()
pyplot.show()
\end{python}

\image{hydrogen-density-s-3}

This is hardly useful, since the density gets extremely small as we get futher from the nucleus. This, however, is the electron density \textit{at a specific point}, while we might be interested in density \textit{of a specific distance from the nucleus}, i.e. density integrated over a sphere of a specific radius. Since the states are spherically symmetric, the integral over a sphere is a value at a point times the area of the sphere \(4\pi r^2\):

\begin{python}
def radial_density(basis, coeffs, r):
    f = density(basis, coeffs, r, 0, 0)
    return f * numpy.pi * 4 * (r**2)
\end{python}

\begin{python}
R = numpy.linspace(0, L, N)

for k in range(3):
    state = radial_density(basis, coeffs[:,k], R)
    pyplot.plot(R, state, label="E = {0:.5f}".format(energy[k]))
\end{python}

\image{hydrogen-radial-s-3}

This plot is my favorite concerning s-orbitals. You can easily see where are those orbitals localized in space, where are the sign changes, etc. The distance of maximal probability for the 1s-orbital is known as \href{https://en.wikipedia.org/wiki/Bohr_radius}{Bohr radius} (in atomic units it is exactly equal to 1).

Let's try some p-orbitals!

\codeonline{hydrogen-p.py}

Hydrogen (and hydrogen-like) orbitals are very special. While s-orbitals are spherically symmetric, p-orbitals are symmetric with respect to rotations around a particular axis and antisymmetric (change sign) upon reflection through that axis. As an example, the p\textsubscript{x}-orbital doesn't change when rotated around the X-axis and flips sign when the X-coordinate changes sign.\footnote{There's much more to it: hydrogen orbitals correspond to irreducible representations of the group \(SO(3)\) of rotations in three dimensions.}

All this, together with basic group representation theory, lets us pick specific basis functions for specific orbitals based on symmetry. We have already used spherically symmetric basis functions for s-orbitals, now we should pick appropriate ones for p-orbitals. Taking HGTO's with \texttt{degree} equal to \texttt{[1,0,0]} seems reasonable: they have the same symmetries as p\textsubscript{x}-orbitals.

Change the \texttt{make\_basis} function to

\begin{python}
def make_basis(params):
    basis = []

    for sigma in params:
        basis.append(hgto.HGTO(sigma, [1,0,0]))

    return basis
\end{python}

Optimize again for the lowest energy:

\begin{python}
def target(params):
    basis = make_basis(params)
    energy, coeffs = solve(*equation(basis))
    return energy[0]
\end{python}

As before, pick some random starting sigmas and optimize:

\begin{python}
params = optimize(numpy.random.uniform(0.5, 5.0, 5))

basis = make_basis(params)
energy, coeffs = solve(*equation(basis))
print("Energy:", list(energy))
\end{python}

There are no 1p-orbitals, only 2p, 3p, etc. This means that the lowest energy value we should get is \(-0.125\):

\begin{console}
Energy: [-0.124990568560368, -0.04460409913403034, 0.10989269961085461,
 0.7056560089020736, 3.5767938893908897]
\end{console}

The first value is surprisingly close to the real answer. Let's use the same trick of optimizing the sum of a few first energies to get more orbitals:

\begin{python}
def target(params):
    basis = make_basis(params)
    energy, coeffs = solve(*equation(basis))
    return sum(energy[0:3])
\end{python}

\begin{console}
Energy: [-0.1248431191321489, -0.05541009666729667, -0.031146344230485228,
 0.0501113155120981, 0.6710920124828863]
\end{console}

Again, the precision of each orbital is questionable compared to actual values of \(-0.125\), \(-\frac{1}{18}=-0.0(5)\), and \(-\frac{1}{32}=-0.03125\), but we got three p-orbitals (2p\textsubscript{x}, 3p\textsubscript{x}, and 4p\textsubscript{x}) using only five basis functions! We may plot them along the X axis, as before:

\begin{python}
L = 30
N = 1001
X = numpy.linspace(-L, L, N)

for k in range(3):
    state = wavefunction(basis, coeffs[:,k], X, 0, 0)
    pyplot.plot(X, state, label="E = {0:.5f}".format(energy[k]))
\end{python}

\image{hydrogen-wavefunction-p-3}

We can plot the density, as well:

\image{hydrogen-density-p-3}

Radial density doesn't make much sense in this case, since p-orbitals are not spherically symmetric.

You may try getting more sophisticated orbitals (3d, 4d, 4f, ...) with higher \texttt{degree} of HGTO's, but the basis starts being close to degenerate, and \texttt{numpy.linalg.LinAlgError} appears more and more.\footnote{As I've said before, it is possible to overcome this using SVD, but this is out of the scope of this book.} At this point, we move on to other systems.

\newpage

\section{Helium cation \(\molecule{He}^+\)}

The next one-electron system we are going to explore is the helium +1 cation --- a helium atom that lost one of its two electrons. This one is very similar to the hydrogen atom, the only difference being the nucleus charge. The Hamiltonian changes to

\begin{equation}
\hat H\psi = -\frac{1}{2} \Delta \psi - \frac{Z}{|r|}\psi
\end{equation}

 Look at \href{https://en.wikipedia.org/wiki/Hydrogen-like_atom\#Non-relativistic_wavefunction_and_energy}{wikipedia} again to see what the energies are: \(-\frac{Z^2}{2n^2}\), where \(Z\) is the charge of the nucleus: \(Z=1\) for hydrogen, \(Z=2\) for helium. Thus, we expect the energies to be 4 times the energies for hydrogen.

\codeonline{helium-s.py}

Change the nucleus charge to 2:

\begin{python}
def equation(basis):
    ...
    n = hgto.Nucleus(2.0)
    ...
\end{python}

to get

\begin{console}
Energy: [-1.990491312820971, -0.49848433212591076, -0.22171347050750198,
 2.3459781841680547, 30.19206227078643]
\end{console}

The three first energies are close to being right. Plotting the radial density

\image{helium-radial-s-3}

we see that the orbitals are closer to the nucleus. This makes sense: the higher the nuclear charge, the stronger is the nucleus-electron attraction. This leads to an interesting phenomenon: all atoms are \textit{roughly} of the same size! Larger atomic number means higher nucleus charge, so the electrons are attracted more and fly closer to the nucleus; however, more electrons means they repel each other and don't concentrate near the nucleus.\footnote{This absence of electron concentration is also largely due to the \href{https://en.wikipedia.org/wiki/Pauli_exclusion_principle}{Pauli principle}, in turn related to the fact that electrons are fermions. Bosons, however, \textit{can} concentrate in one place, which leads to the existence of \href{https://en.wikipedia.org/wiki/Bose\%E2\%80\%93Einstein_condensate}{Bose-Einstein condensate}.}

We can find and plot the p-orbitals as well, which will again look the same as hydrogen p-orbitals, but closer to the nucleus and having higher energy.

Everything interesting related to helium starts when we add a second electron. Since this book considers one-electron systems only, we move on.\footnote{A \textit{rough} description of what happens in a real helium atom is that the two-electron density looks a lot like the 1s-orbital, but slightly more spread out due to electron-electron repulsion. Pauli exclusion principle doesn't change anything in two-electron systems: we have two electrons and two possible spins as well. See also \href{https://twitter.com/lisyarus/status/1187373256178913282?s=20}{here} and \href{https://twitter.com/lisyarus/status/1188908501436960770?s=20}{there}.}

\newpage

\section{Lithium 2+ cation \(\molecule{Li}^{2+}\)}

This is, again, almost the same as hydrogen. Lithium nucleus has charge +3, and the lithium atom has 3 electrons at normal conditions. If we steal two of them, we get the lithium +2 cation, which is described the same way as helium, but with \(Z=3\).

\codeonline{lithium-s.py}

Change the nucleus charge to 3:

\begin{python}
def equation(basis):
    ...
    n = hgto.Nucleus(3.0)
    ...
\end{python}

and get something like

\begin{console}
Energy: [-4.478605460280523, -1.1215897686285095, -0.49885528082305414,
 5.2782304665358195, 67.92747569252208]
\end{console}

Close enough to the true \(-4.5\), \(-1.125\), and \(-0.5\). The orbitals are even closer to the nucleus this time:

\image{lithium-radial-s-3}

Excercise: guess the dependence of orbitals' spread on the value of \(Z\), then look at \href{https://en.wikipedia.org/wiki/Hydrogen-like_atom\#Non-relativistic_wavefunction_and_energy}{wikipedia} one more time and see if you guessed right.\footnote{Hint: look at the exponential term of the radial part of the wavefunction.}

One can proceed with all atoms the same way, but this is starting to get dull. More importantly, these one-electron atoms are rare in nature: it takes more and more energy\footnote{See also \href{https://en.wikipedia.org/wiki/Molar_ionization_energies_of_the_elements}{here}.} to remove all electrons but one from a heavy atom. We haven't seen \textit{chemical bonding} yet --- the time has come.

\newpage

\section{Dihydrogen cation \(\molecule{H}_2^+\)}
\section{Trihydrogen 2+ cation \(\molecule{H}_3^{2+}\)}
\section{Helium hydride 2+ cation \(\molecule{HeH}^{2+}\)}

\begin{thebibliography}{9}
\bibitem{ref:atkins}
	Peter Atkins, Ronald Friedman,
	\textit{Molecular Quantum Mechanics}
\bibitem{ref:lieb}
	Elliot Lieb,
	\textit{The Stability of Matter: From Atoms to Stars}
\bibitem{ref:hall}
	Brian Hall,
	\textit{Quantum Theory for Mathematicians}
\bibitem{ref:kato-th}
	Tosio Kato,
	\textit{On the eigenfunctions of many-particle systems in quantum mechanics}
\bibitem{ref:kato-ham}
	Tosio Kato,
	\textit{Fundamental properties of Hamiltonian operators of Schrödinger type}
\bibitem{ref:fulton-harris}
	William Fulton, Joe Harris,
	\textit{Representation Theory: A First Course}
\bibitem{ref:edmonds}
	A. R. Edmonds,
	\textit{Angular Momentum in Quantum Mechanics}
\bibitem{ref:tmpchem}
	TMP Chem,
	\url{www.youtube.com/channel/UC3dZQdfv67X49cZkoXWYSwQ}
\bibitem{ref:psi4numpy}
	Psi4NumPy,
	\url{www.github.com/psi4/psi4numpy}
\end{thebibliography}

\begin{appendices}
\section{Hermite polynomials} \label{apx:hermite}
\section{Derivation of GTO integrals} \label{apx:integrals}
\end{appendices}

\end{document}
